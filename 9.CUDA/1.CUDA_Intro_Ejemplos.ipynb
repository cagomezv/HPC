{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS0SrJNBNb5Q",
        "outputId": "c9c067d7-5a25-4a8c-a661-32c4d85ec8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin  colab  cuda  cuda-12  cuda-12.2  etc  games  include  lib\tlib64  man  opt  sbin  share  src\n"
          ]
        }
      ],
      "source": [
        "# Vemos si tenemos instalado CUDA\n",
        "! dir /usr/local/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para checar la versión del compilador de CUDA\n",
        "! nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQekAUmBOBED",
        "outputId": "f17cbf1a-7c2a-4d9b-bd0f-b563a16c7a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo_1 HelloCUDA\n",
        "%%writefile HelloCUDA.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void helloCUDA(float e){\n",
        "    printf(\"Soy el hilo %d del bloque %d con valor e=%f\\n\",threadIdx.x,blockIdx.x,e);\n",
        "}\n",
        "\n",
        "int main(void){\n",
        "    printf(\"\\nHello World\\n\");\n",
        "\n",
        "    helloCUDA<<<4,4>>>(2.5f);\n",
        "\n",
        "    cudaDeviceReset();\n",
        "    return(0);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz7hb-QLOG_O",
        "outputId": "27d707ed-cda4-4320-89b5-d8dfd7da3c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting HelloCUDA.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilando con nvcc\n",
        "! nvcc HelloCUDA.cu -o HelloCUDA"
      ],
      "metadata": {
        "id": "XUmKrbYaO3Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutando el programa\n",
        "! ./HelloCUDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgIMZEHAPu23",
        "outputId": "25159bdd-b94f-4eb7-e397-e7e637b5a71d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hello World\n",
            "Soy el hilo 0 del bloque 1 con valor e=2.500000\n",
            "Soy el hilo 1 del bloque 1 con valor e=2.500000\n",
            "Soy el hilo 2 del bloque 1 con valor e=2.500000\n",
            "Soy el hilo 3 del bloque 1 con valor e=2.500000\n",
            "Soy el hilo 0 del bloque 3 con valor e=2.500000\n",
            "Soy el hilo 1 del bloque 3 con valor e=2.500000\n",
            "Soy el hilo 2 del bloque 3 con valor e=2.500000\n",
            "Soy el hilo 3 del bloque 3 con valor e=2.500000\n",
            "Soy el hilo 0 del bloque 2 con valor e=2.500000\n",
            "Soy el hilo 1 del bloque 2 con valor e=2.500000\n",
            "Soy el hilo 2 del bloque 2 con valor e=2.500000\n",
            "Soy el hilo 3 del bloque 2 con valor e=2.500000\n",
            "Soy el hilo 0 del bloque 0 con valor e=2.500000\n",
            "Soy el hilo 1 del bloque 0 con valor e=2.500000\n",
            "Soy el hilo 2 del bloque 0 con valor e=2.500000\n",
            "Soy el hilo 3 del bloque 0 con valor e=2.500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo_2 suma_vectores\n",
        "%%writefile suma_vectores.cu\n",
        "/*\n",
        "Programa:      Suma_Vectores.cu\n",
        "Descripción:   Dados dos vectores de tamaño N, sumarlos en paralelo y guardar el resultado en un tercer vector.\n",
        "Actualización: 09/Jul/2020\n",
        "*/\n",
        "\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Función Kernel que se ejecuta en el Device.\n",
        "__global__ void Suma_vectores(float *c_d,float *a_d,float *b_d, int N)\n",
        "{\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx<N){\n",
        "\t  c_d[idx] = a_d[idx] + b_d[idx];\n",
        "  }\n",
        "}\n",
        "\n",
        "// Código principal que se ejecuta en el Host\n",
        "int main(void){\n",
        "\tfloat *a_h,*b_h,*c_h; //Punteros a arreglos en el Host\n",
        "\tfloat *a_d,*b_d,*c_d;  //Punteros a arreglos en el Device\n",
        "\tconst int N = 24;  //Número de elementos en los arreglos  (probar 1000000)\n",
        "\n",
        "\tsize_t size=N * sizeof(float);\n",
        "\n",
        "\ta_h = (float *)malloc(size); // Pedimos memoria en el Host\n",
        "\tb_h = (float *)malloc(size);\n",
        "\tc_h = (float *)malloc(size);//También se puede con cudaMallocHost\n",
        "\n",
        "\t//Inicializamos los arreglos a,b en el Host\n",
        "\tsrand(time(NULL));\n",
        "\tfor (int i=0; i<N; i++){\n",
        "\t\t//a_h[i] = (float)i;b_h[i] = (float)(i+1);\n",
        "\t\ta_h[i] = rand() % 100 + 1.0;\n",
        "\t\tb_h[i] = rand() % 100 + 1.0;\n",
        "\t}\n",
        "\n",
        "\tprintf(\"\\nArreglo a:\\n\");\n",
        "\tfor (int i=0; i<N; i++) printf(\"%f \", a_h[i]);\n",
        "\tprintf(\"\\n\\nArreglo b:\\n\");\n",
        "\tfor (int i=0; i<N; i++) printf(\"%f \", b_h[i]);\n",
        "\n",
        "\tcudaMalloc((void **) &a_d,size);   // Pedimos memoria en el Device\n",
        "\tcudaMalloc((void **) &b_d,size);\n",
        "\tcudaMalloc((void **) &c_d,size);\n",
        "\n",
        "\t//Pasamos los arreglos a y b del Host al Device\n",
        "\tcudaMemcpy(a_d, a_h, size, cudaMemcpyHostToDevice);\n",
        "\tcudaMemcpy(b_d, b_h, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//Realizamos el cálculo en el Device\n",
        "\tint block_size =8;\n",
        "\tint n_blocks = N/block_size + (N%block_size == 0 ? 0:1);\n",
        "\n",
        "\tSuma_vectores <<< n_blocks, block_size >>> (c_d,a_d,b_d,N);\n",
        "\n",
        "\t//Pasamos el resultado del Device al Host\n",
        "\tcudaMemcpy(c_h, c_d, size,cudaMemcpyDeviceToHost);\n",
        "\n",
        "\t//Resultado\n",
        "\tprintf(\"\\n\\nArreglo c:\\n\");\n",
        "\tfor (int i=0; i<N; i++) printf(\"%f \", c_h[i]);\n",
        "\n",
        "\tprintf(\"\\n\\nFin del programa...\\n\");\n",
        "\t//system(\"pause\");\n",
        "\n",
        "\t// Liberamos la memoria del Host\n",
        "\tfree(a_h);\n",
        "\tfree(b_h);\n",
        "\tfree(c_h);\n",
        "\n",
        "\t// Liberamos la memoria del Device\n",
        "\tcudaFree(a_d);\n",
        "\tcudaFree(b_d);\n",
        "\tcudaFree(c_d);\n",
        "\treturn(0);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afw5pyuvP6A_",
        "outputId": "52533d09-f246-4323-c7b7-8d10457a6b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma_vectores.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc suma_vectores.cu -o suma_vectores"
      ],
      "metadata": {
        "id": "Tvd6tGEHQJy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./suma_vectores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN-hj9yJQO-j",
        "outputId": "c2bd157c-626f-4dec-8678-52a248ebb1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Arreglo a:\n",
            "8.000000 36.000000 32.000000 79.000000 34.000000 43.000000 4.000000 96.000000 39.000000 83.000000 57.000000 22.000000 36.000000 64.000000 52.000000 58.000000 99.000000 59.000000 22.000000 42.000000 27.000000 44.000000 69.000000 87.000000 \n",
            "\n",
            "Arreglo b:\n",
            "16.000000 48.000000 78.000000 12.000000 38.000000 89.000000 33.000000 6.000000 8.000000 61.000000 27.000000 24.000000 74.000000 39.000000 32.000000 59.000000 93.000000 31.000000 37.000000 55.000000 37.000000 30.000000 91.000000 7.000000 \n",
            "\n",
            "Arreglo c:\n",
            "24.000000 84.000000 110.000000 91.000000 72.000000 132.000000 37.000000 102.000000 47.000000 144.000000 84.000000 46.000000 110.000000 103.000000 84.000000 117.000000 192.000000 90.000000 59.000000 97.000000 64.000000 74.000000 160.000000 94.000000 \n",
            "\n",
            "Fin del programa...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo_3 Sumatoria\n",
        "%%writefile kernel.h\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "//#include <time.h>\n",
        "double sumOfArrayGPU(double* A, long int n);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM--8XUdQasU",
        "outputId": "c69c4d4a-d543-4f25-e79d-4aff314ecd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kernel.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel.cu\n",
        "#include \"kernel.h\"\n",
        "\n",
        "#define TPB 1024\n",
        "#define ATOMIC 1 // 0 para no usar el atomicAdd\n",
        "\n",
        "__global__ void sumOfArrayKernel(double *d_sum_total, double *d_A, long int n) {\n",
        "\tconst long int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "\tconst int s_idx = threadIdx.x;\n",
        "\t__shared__ double s_data[TPB];\n",
        "\n",
        "\ts_data[s_idx] = (idx<n) ? d_A[idx] : 0.0;//Util cuando n/TPB no es entero\n",
        "\t__syncthreads();\n",
        "\n",
        "\tif (s_idx==0) {\n",
        "\t\tdouble blockSum = 0.0;\n",
        "\t\tfor (int j = 0; j < blockDim.x; j++) {\n",
        "\t\t\tblockSum += s_data[j];\n",
        "\t\t}\n",
        "\t\t//printf(\"Block_%d, blockSum = %lf\\n\", blockIdx.x, blockSum);\n",
        "\t\tif (ATOMIC) {\n",
        "\t\t\tatomicAdd(d_sum_total, blockSum);//Hay que asegurarnos de tener una tarjeta con capacidad >= 6.0 para usar double en el atomicAdd\n",
        "\t\t\t                                 //Mi GPU tiene capaciad 5.2, por lo tanto no va a reconocer el atomicAdd con valores double\n",
        "\t\t\t                                 //Vamos a probar en la GPU Quadro 8000 que tiene capacidad 7.5\n",
        "\t\t}\n",
        "\t\telse {\n",
        "\t\t\t*d_sum_total += blockSum; //Resultados no esperados\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void sumOfArrayKernel_V2(double* d_sum_total, double* d_A, long int n) {\n",
        "\tconst long int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "\n",
        "\tdouble blockSum = 0.0;\n",
        "\tconst int s_idx = threadIdx.x;\n",
        "\t__shared__ double s_data[TPB];\n",
        "\n",
        "\ts_data[s_idx]= blockSum = (idx < n) ? d_A[idx] : 0.0;\n",
        "\n",
        "\t__syncthreads();\n",
        "\n",
        "\tfor (unsigned int s = blockDim.x / 2; s > 0; s >>= 1)\n",
        "\t{\n",
        "\t\tif (s_idx < s)\n",
        "\t\t{\n",
        "\t\t\ts_data[s_idx] = blockSum = blockSum + s_data[s_idx + s];\n",
        "\t\t}\n",
        "\n",
        "\t\t__syncthreads();\n",
        "\t}\n",
        "\n",
        "\tif (s_idx == 0) {\n",
        "\t\tif (ATOMIC) {\n",
        "\t\t\tatomicAdd(d_sum_total, blockSum);\n",
        "\t\t}\n",
        "\t\telse {\n",
        "\t\t\t*d_sum_total += blockSum; //Resultados no esperados\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "double sumOfArrayGPU(double *A, long int n){\n",
        "\tdouble *d_A;\n",
        "\tdouble *d_sum_total;\n",
        "\tdouble sum_total;\n",
        "\t//double t_ini, t_fin,time_kernel;\n",
        "\n",
        "\t//1. Crear memoria en la GPU\n",
        "\tcudaMalloc(&d_sum_total, sizeof(double));\n",
        "\tcudaMalloc(&d_A, n * sizeof(double));\n",
        "\n",
        "\t//Inicializamos en cero\n",
        "\tcudaMemset(d_sum_total, 0, sizeof(double));\n",
        "\n",
        "\t//2. Copiar memoria (CPU-->GPU)\n",
        "\tcudaMemcpy(d_A, A, n * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//t_ini = clock();\n",
        "\t//3. Ejecutar función Kernel\n",
        "\tsumOfArrayKernel <<<(n+TPB-1)/TPB,TPB >>> (d_sum_total,d_A,n);\n",
        "\t//sumOfArrayKernel_V2 << <(n + TPB - 1) / TPB, TPB >> > (d_sum_total, d_A, n);\n",
        "\t/*cudaDeviceSynchronize();\n",
        "\tt_fin = clock();\n",
        "\ttime_kernel = (t_fin - t_ini) / CLOCKS_PER_SEC;\n",
        "\tprintf(\"\\nTiempo de procesamiento del Kernel: %lf seconds\\n\", time_kernel);\n",
        "\t*/\n",
        "\t//4. Copiar memoria (GPU-->CPU)\n",
        "\tcudaMemcpy(&sum_total, d_sum_total, sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "\tcudaFree(d_sum_total);\n",
        "\tcudaFree(d_A);\n",
        "\tcudaDeviceReset();\n",
        "\treturn(sum_total);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIoh1IMtQ3gW",
        "outputId": "04c32eca-4c66-4472-ee8c-f767a0ce1c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sumatoria_main.cpp\n",
        "/*\n",
        "Programa:       sumatoria_main.cpp\n",
        "Descripción:    Dado un arreglo de elementos en punto flotante (double) de tamaño N\n",
        "                calcular la sumatoria de todos los elementos del arreglo\n",
        "Actualización:  07/Jul/2020\n",
        "*/\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <math.h>\n",
        "\n",
        "/*CUDA*/\n",
        "#include \"kernel.h\"\n",
        "\n",
        "double* generateRandomArray(long int numberOfElements);\n",
        "double sumOfArraySeq(double* A, long int numberOfElements);\n",
        "void printArray(double* A, long int n);\n",
        "\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    double t_ini, t_fin;\n",
        "    double time_generateData,time_cpu_seconds,time_gpu_seconds;\n",
        "    double *A;\n",
        "    double sum_total_cpu,sum_total_gpu;\n",
        "    long int n=500000000;\n",
        "\n",
        "    printf(\"De cuantos elementos son los arreglos\\n\");\n",
        "    scanf(\"%ld\", &n);\n",
        "\n",
        "    t_ini = clock();\n",
        "    A = generateRandomArray(n);\n",
        "    t_fin = clock();\n",
        "    time_generateData = (t_fin - t_ini) / CLOCKS_PER_SEC;\n",
        "    //printArray(A,n);\n",
        "    //\n",
        "    t_ini = clock();\n",
        "    sum_total_cpu = sumOfArraySeq(A, n);\n",
        "    t_fin = clock();\n",
        "    time_cpu_seconds = (t_fin - t_ini) / CLOCKS_PER_SEC;\n",
        "\n",
        "    t_ini = clock();\n",
        "    sum_total_gpu = sumOfArrayGPU(A,n);\n",
        "    t_fin = clock();\n",
        "    time_gpu_seconds = (t_fin - t_ini) / CLOCKS_PER_SEC;\n",
        "\n",
        "    printf(\"La suma del arreglo en CPU es: %lf \\n\", sum_total_cpu);\n",
        "    printf(\"La suma del arreglo en GPU es: %lf \\n\", sum_total_gpu);\n",
        "    printf(\"Tiempo para generar datos: %lf segundos.\\n\", time_generateData);\n",
        "    printf(\"Tiempo de procesamiento en CPU: %lf segundos.\\n\",time_cpu_seconds);\n",
        "    printf(\"Tiempo de procesamiento en GPU: %lf segundos.\\n\", time_gpu_seconds);\n",
        "\n",
        "    free(A);\n",
        "}\n",
        "\n",
        "double* generateRandomArray(long int numberOfElements) {\n",
        "    double* myArray;\n",
        "    //srand(time(NULL));\n",
        "    srand(1);\n",
        "    myArray = (double *)malloc(numberOfElements * sizeof(double));\n",
        "    for (long int i = 0; i < numberOfElements; i++) {\n",
        "        myArray[i] = rand() % 100 + 1.0;\n",
        "        //myArray[i] = 1.0;\n",
        "    }\n",
        "    return myArray;\n",
        "}\n",
        "\n",
        "void printArray(double* A, long int n) {\n",
        "\n",
        "    printf(\"Arreglo=\");\n",
        "    for (long int i = 0; i < n; i++) {\n",
        "        printf(\"%f  \", A[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "double sumOfArraySeq(double* A, long int numberOfElements) {\n",
        "    double resultSum = 0.0;\n",
        "    for (long int i = 0; i < numberOfElements; i++) {\n",
        "        resultSum = resultSum + A[i];\n",
        "    }\n",
        "    return resultSum;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaCY40K9RDTU",
        "outputId": "c6aeda33-61cf-40cd-9637-e271a1dc60fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sumatoria_main.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilando usando -arch para especificar la arquitectura de la GPU\n",
        "! nvcc -arch=compute_75 kernel.cu sumatoria_main.cpp -o run_sumatoria_V1"
      ],
      "metadata": {
        "id": "9xmVJvRORQcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando el perfilador (profiler) de nvidia: nvprof\n",
        "! nvprof ./run_sumatoria_V1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO3HpU2ORtLL",
        "outputId": "f76fd3e4-9e86-475a-cb5c-1ab20d81679a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "De cuantos elementos son los arreglos\n",
            "1000000000\n",
            "==1608== NVPROF is profiling process 1608, command: ./run_sumatoria_V1\n",
            "La suma del arreglo en CPU es: 50501072177.000000 \n",
            "La suma del arreglo en GPU es: 50501072177.000000 \n",
            "Tiempo para generar datos: 23.084276 segundos.\n",
            "Tiempo de procesamiento en CPU: 3.073492 segundos.\n",
            "Tiempo de procesamiento en GPU: 2.962195 segundos.\n",
            "==1608== Profiling application: ./run_sumatoria_V1\n",
            "==1608== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   64.56%  1.67953s         1  1.67953s  1.67953s  1.67953s  [CUDA memcpy HtoD]\n",
            "                   35.44%  921.85ms         1  921.85ms  921.85ms  921.85ms  sumOfArrayKernel(double*, double*, long)\n",
            "                    0.00%  1.9520us         1  1.9520us  1.9520us  1.9520us  [CUDA memset]\n",
            "                    0.00%  1.8240us         1  1.8240us  1.8240us  1.8240us  [CUDA memcpy DtoH]\n",
            "      API calls:   88.61%  2.60171s         2  1.30085s  921.88ms  1.67983s  cudaMemcpy\n",
            "                    8.13%  238.61ms         2  119.31ms  7.0020ms  231.61ms  cudaMalloc\n",
            "                    3.03%  89.081ms         1  89.081ms  89.081ms  89.081ms  cudaDeviceReset\n",
            "                    0.22%  6.5944ms         2  3.2972ms  177.01us  6.4174ms  cudaFree\n",
            "                    0.00%  118.54us       101  1.1730us     136ns  49.756us  cuDeviceGetAttribute\n",
            "                    0.00%  45.456us         1  45.456us  45.456us  45.456us  cudaLaunchKernel\n",
            "                    0.00%  25.134us         1  25.134us  25.134us  25.134us  cuDeviceGetName\n",
            "                    0.00%  18.990us         1  18.990us  18.990us  18.990us  cudaMemset\n",
            "                    0.00%  5.2950us         1  5.2950us  5.2950us  5.2950us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7260us         3     575ns     255ns  1.2090us  cuDeviceGetCount\n",
            "                    0.00%     961ns         2     480ns     213ns     748ns  cuDeviceGet\n",
            "                    0.00%     546ns         1     546ns     546ns     546ns  cuModuleGetLoadingMode\n",
            "                    0.00%     429ns         1     429ns     429ns     429ns  cuDeviceTotalMem\n",
            "                    0.00%     260ns         1     260ns     260ns     260ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegurarse de descomentar la versión V2:\n",
        "# //sumOfArrayKernel <<<(n+TPB-1)/TPB,TPB >>> (d_sum_total,d_A,n);\n",
        "#\tsumOfArrayKernel_V2 << <(n + TPB - 1) / TPB, TPB >> > (d_sum_total, d_A, n);\n",
        "\n",
        "! nvcc -arch=compute_75 kernel.cu sumatoria_main.cpp -o run_sumatoria_V2"
      ],
      "metadata": {
        "id": "AuZn1OXbDLY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvprof ./run_sumatoria_V2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDKYCG4ySSEi",
        "outputId": "e0c81253-595a-4741-ab07-973072f5d50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "De cuantos elementos son los arreglos\n",
            "1000000000\n",
            "==2090== NVPROF is profiling process 2090, command: ./run_sumatoria_V2\n",
            "La suma del arreglo en CPU es: 50501072177.000000 \n",
            "La suma del arreglo en GPU es: 50501072177.000000 \n",
            "Tiempo para generar datos: 22.214245 segundos.\n",
            "Tiempo de procesamiento en CPU: 3.228597 segundos.\n",
            "Tiempo de procesamiento en GPU: 2.267603 segundos.\n",
            "==2090== Profiling application: ./run_sumatoria_V2\n",
            "==2090== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   87.54%  1.68190s         1  1.68190s  1.68190s  1.68190s  [CUDA memcpy HtoD]\n",
            "                   12.46%  239.48ms         1  239.48ms  239.48ms  239.48ms  sumOfArrayKernel_V2(double*, double*, long)\n",
            "                    0.00%  2.1760us         1  2.1760us  2.1760us  2.1760us  [CUDA memcpy DtoH]\n",
            "                    0.00%  1.9520us         1  1.9520us  1.9520us  1.9520us  [CUDA memset]\n",
            "      API calls:   85.59%  1.92171s         2  960.85ms  239.51ms  1.68219s  cudaMemcpy\n",
            "                   10.15%  227.89ms         2  113.94ms  7.2408ms  220.65ms  cudaMalloc\n",
            "                    3.98%  89.323ms         1  89.323ms  89.323ms  89.323ms  cudaDeviceReset\n",
            "                    0.27%  6.0004ms         2  3.0002ms  177.26us  5.8231ms  cudaFree\n",
            "                    0.01%  124.08us       101  1.2280us     134ns  49.832us  cuDeviceGetAttribute\n",
            "                    0.00%  40.039us         1  40.039us  40.039us  40.039us  cudaLaunchKernel\n",
            "                    0.00%  29.006us         1  29.006us  29.006us  29.006us  cudaMemset\n",
            "                    0.00%  26.545us         1  26.545us  26.545us  26.545us  cuDeviceGetName\n",
            "                    0.00%  6.0990us         1  6.0990us  6.0990us  6.0990us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6390us         3     546ns     191ns  1.1500us  cuDeviceGetCount\n",
            "                    0.00%  1.0040us         2     502ns     336ns     668ns  cuDeviceGet\n",
            "                    0.00%     516ns         1     516ns     516ns     516ns  cuModuleGetLoadingMode\n",
            "                    0.00%     411ns         1     411ns     411ns     411ns  cuDeviceTotalMem\n",
            "                    0.00%     229ns         1     229ns     229ns     229ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ]
}